model_name:
tokenizer_name:
model_type: mlm
optimizer: adamw
learning_rate: !!float 1e-4
weight_decay: !!float 1e-4
batch_size: 32
data_path: data
seq_length: 512
num_workers: 0
accumulate_grad_batches: 1
gradient_clip_val:
max_epochs: 3
steps_per_epoch:
wandb_name:
project: mlm
seed:
fast_dev_run: 0
save_path: "save/my_model"
log_every_n_steps: 100
resume_from_checkpoint:
